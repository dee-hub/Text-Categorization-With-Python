{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320653a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dee/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import ttk\n",
    "import center_tk_window\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection, metrics\n",
    "\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "## for bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cecf2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install center_tk_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c55d8",
   "metadata": {},
   "source": [
    "## English GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3126b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cc0030e4f34fad8d24577acbe02ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3271fe4b3a4346a384e2004bd7e76e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#Loading Naive Bayes for all languages\n",
    "#English\n",
    "e = open('model/NB_model.pickle', 'rb')\n",
    "nb_english = pickle.load(e)\n",
    "e.close()\n",
    "#Yoruba\n",
    "y = open('model/NB_model_yoruba.pickle', 'rb')\n",
    "nb_yoruba = pickle.load(y)\n",
    "y.close()\n",
    "#Hausa\n",
    "h = open('model/NB_model_hausa.pickle', 'rb')\n",
    "nb_hausa = pickle.load(h)\n",
    "h.close()\n",
    "\n",
    "#Loading LSTM\n",
    "#English\n",
    "from keras.models import load_model\n",
    "LSTM_english = load_model('model/LSTM.h5')\n",
    "#Yoruba\n",
    "LSTM_yoruba = load_model('model/LSTM_yoruba.h5')\n",
    "#Hausa\n",
    "LSTM_hausa = load_model('model/LSTM_hausa.h5')\n",
    "\n",
    "#Load BERT\n",
    "import torch\n",
    "BERT_english = torch.load('model/model')\n",
    "\n",
    "win = tk.Tk()\n",
    "win.geometry(\"840x640\")\n",
    "win.title('Multiclass Text Classification')\n",
    "win.resizable(False, False)\n",
    "center_tk_window.center_on_screen(win)\n",
    "font = (\"Segoe Boot Semilight\", 9)\n",
    "\n",
    "def getTextInputEnglish():\n",
    "    result=scr.get(\"1.0\",\"end\")\n",
    "        #NB\n",
    "    NB = nb_english.predict([result])\n",
    "    if NB == ['BUSINESS']:\n",
    "        predictedNBEnglish = 'BUSINESS'\n",
    "    elif NB == ['ENTERTAINMENT']:\n",
    "        predictedNBEnglish = 'ENTERTAINMENT'\n",
    "    elif NB == ['POLITICS']:\n",
    "        predictedNBEnglish = 'POLITICS'\n",
    "    elif NB == ['SCIENCE&TECH']:\n",
    "        predictedNBEnglish = 'SCIENCE&TECH'\n",
    "    #LSTM\n",
    "    MAX_NB_WORDS = 50000\n",
    "    # Max number of words in each complaint.\n",
    "    MAX_SEQUENCE_LENGTH = 250\n",
    "    # This is fixed.\n",
    "    EMBEDDING_DIM = 100\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    seq = tokenizer.texts_to_sequences([result])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    pred = LSTM_english.predict(padded)\n",
    "    labels = ['BUSINESS', 'ENTERTAINMENT', 'POLITICS', 'SCIENCE&TECH']\n",
    "    predictedLSTMEnglish = labels[np.argmax(pred)]\n",
    "    \n",
    "    #BERT\n",
    "    bert, raw_outputs = BERT_english.predict([result])\n",
    "    if bert == [0]:\n",
    "        predictedBERT = 'BUSINESS'\n",
    "    elif bert == [1]:\n",
    "        predictedBERT = 'ENTERTAINMENT'\n",
    "    elif bert == [2]:\n",
    "        predictedBERT = 'POLITICS'\n",
    "    elif bert == [3]:\n",
    "        predictedBERT = 'SCIENCE&TECH'\n",
    "    print(result)\n",
    "    #'BUSINESS', 'ENTERTAINMENT', 'POLITICS', 'SCIENCE&TECH'\n",
    "    all_4points_text = \"Naive Bayes: \" + str(predictedNBEnglish) + \"\\n\\n\" + \"LSTM: \" + str(predictedLSTMEnglish) + \"\\n\\n\" + \"BERT Transformer : \" + str(predictedBERT) + \"\\n\\n\" + \"....................................... \\n\\n\"\n",
    "    text.insert(tk.END, all_4points_text)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#radVar = tk.IntVar()\n",
    "#English = tk.Radiobutton(win, text='English', variable = radVar, value=1, fg='black',\n",
    "#                            activeforeground='black', selectcolor='white', command=radCall)\n",
    "#English.grid(column=0, row=0, sticky=tk.W, padx=(370, 0))\n",
    "#Yoruba = tk.Radiobutton(win, text='Yoruba', variable = radVar, value=2, fg='black',\n",
    " #                     activeforeground='black', selectcolor='white', command = radCall)\n",
    "#Yoruba.grid(column=0, row=0, sticky=tk.W, padx=(480,0))\n",
    "\n",
    "#Hausa = tk.Radiobutton(win, text='Hausa', variable = radVar, value=3, fg='black',\n",
    " #                     activeforeground='black', selectcolor='white', command = radCall)\n",
    "#Hausa.grid(column=0, row=0, sticky=tk.W, padx=(590,0))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "ttk.Label(win, text=\"Enter your text\", font=font).grid(column=0, row=0)\n",
    "scrol_w = 117\n",
    "scrol_h = 23\n",
    "scr = scrolledtext.ScrolledText(win, width=scrol_w, height=scrol_h, wrap=tk.WORD)\n",
    "scr.grid(column=0, row=1, columnspan=3)\n",
    "   \n",
    "btnRead=tk.Button(win, height=1, width=10, text=\"Classify\", command=getTextInputEnglish)\n",
    "btnRead.grid(column=0, row=2, padx=(5), pady=(20,0))\n",
    "\n",
    "text=scrolledtext.ScrolledText(win, height=14, width=117)\n",
    "\n",
    "text.grid(column=0, row=3, padx=(5), pady=(20,0))\n",
    "\n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194086c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7eff300e3d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dee/anaconda3/lib/python3.8/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-7-a9782ae73949>\", line 63, in getTextInputYoruba\n",
      "    all_4points_text = \"Naive Bayes: \" + str(predictedNB) + \"\\n\\n\" + \"LSTM: \" + str(predictedLSTMEnglish) + \"\\n\\n\" + \"BERT Transformer : \" + str(predictedBERT) + \"\\n\\n\" + \"....................................... \\n\\n\"\n",
      "UnboundLocalError: local variable 'predictedNB' referenced before assignment\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fed2e6c4",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from google_trans_now import google_translator\n",
    "df = pd.read_csv(\"data_cleaned.csv\")\n",
    "df = pd.DataFrame(df, columns=[\"headline\", \"category\", \"text_clean\"])\n",
    "df['category']\n",
    "translator = google_translator()\n",
    "#df['translated_value'] = df['category'].apply(lambda x: translator.translate(x, lang_tgt='en').text)\n",
    "df['translated_value'] = df['text_clean'].apply(lambda x: translator.translate(x, lang_tgt='yo'))\n",
    "    \n",
    "    \n",
    "#print(df1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6018c4d2",
   "metadata": {},
   "source": [
    "a = 'Ìpàdé ikọ̀ Amotekun pẹ̀lú àwọn aládúgbò ọmọ ọdún 15 tí wọ́n yìnbọn pa ní Mokola, ohun tí a gbọ́ níbẹ̀ nìyí'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65569fd5",
   "metadata": {},
   "source": [
    "from google_trans_now import google_translator  \n",
    "  \n",
    "translator = google_translator()  \n",
    "translate_text = translator.translate(a,lang_tgt='en')  \n",
    "print(translate_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4613affd",
   "metadata": {},
   "source": [
    "df['translated_value']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d3f3573",
   "metadata": {},
   "source": [
    "df['headline'].iloc[5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3697c84c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
